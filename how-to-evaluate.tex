\section{Evaluating Program Transformation Tools}
\label{sec:how-to-evaluate}
As we've discussed, evaluating the usefulness of many program
transformation tools requires some subjective judgment. In this
section, we outline possible ways to evaluate such tools.

The designer of the tool will have a usage scenario for the 
transformations produced by the tool. This usage scenario is 
necessary but not sufficient to establish that the tool is useful.
At this level of abstraction, the usage scenario is simply something
that a reader considers so that they can decide, based on their experience, 
whether the tool seems like it would be useful to them or not. 
It is still too abstract to make concrete decisions about.

Typically, a program transformation tool generates a large set of
candidate transformations. Depending on the tool, some of these
transformations may be inappropriate. For instance, a refactoring
tool should only propose semantics-preserving transformations; on the
other hand, we would expect a program repair tool to change the semantics
of the program being repaired. 

In any case, even after ruling out 
inappropriate transformations, the tool would still have a large
number of candidates to consider, and must select some of the candidates to show
to the user. This selection process is a key factor in the usability of the tool.
% cite the tricoder paper
Promising tools may fail to be useful in practice because the selection is not
tuned to users' needs.

A key question is: which humans should evaluate the results of the tool?
Possible answers include the tool developers, arbitrary developers, or the developers
of particular software projects.

Along with ``which humans'', another key question is how to solicit
information from the humans. Questionnaires and user studies are two
ways to ask solicit information. One can ask for abstract opinions
(``would this tool be useful?'') or concrete opinions (``is this
specific proposed transformation useful?'') At the limit, perhaps the
best way to show that a tool in this space is useful in practice is by
submitting merge requests to maintainers of open-source projects---a 
highly specific and idiosyncratic user study which may be hard to generalize.

We will discuss best practices for this type of evaluation.


%% One of the common inadequacies that they found states that, unfortunately: ``(Programming Languages) papers we looked at often subset a benchmark, or cherry-picked particular programs.''. This threatens the validity of the claims. We included the documented process of choosing the benchmarks and benchmark requirement list to avoid this issue in our work.

%% ``An unsound claim can misdirect a field, encouraging the pursuit of unworthy ideas and the abandonment of promising ideas.''. S. M. Blackburn et al.~\cite{truth_Blackburn} believed that having a methodical approach to exploring, exposing, and addressing the root of unsound claims and poor exposition would help to solve this issue. 

\section{Evaluating Program Transformation Tools}
\label{sec:how-to-evaluate}
As we've discussed, evaluating the usefulness of many program
transformation tools requires some subjective judgment. In this
section, we outline possible ways to solicit judgment and evaluate
such tools.

The designer of the tool will have a usage scenario for the
transformations produced by the tool. This usage scenario is necessary
but not sufficient to establish that the tool is useful. It is simply
a first step---at this level of abstraction, the usage scenario is
simply something that a reader considers so that they can decide,
based on their experience, whether the tool seems like it would be
useful to them or not.  It is still too abstract to make concrete
decisions about.

Exploratory, or pilot, studies can be useful to establish that the
anticipated usage scenarios actually occur in the wild. For instance,
we have had the unfortunate experience of designing a static analysis,
and then having a hard time finding code to which it was
applicable. \cite{beckman11:_empir_study_objec_protoc_wild}, on the
other hand, have conducted an empirical study of 15 benchmark programs
and 2 million lines of code, establishing that 7.2\% of types defined
object protocols, and 13\% of classes were clients of such types.
Such examples also serve as concrete examples of a tool's usefulness.

Tool designers can solicit opinions along a continuum from abstract
opinions to concrete judgments. There is a continuum between the
abstract opinion ``would tool T be useful,'' through to ``do you
support this usage scenario X'', all the way to the concrete ``is
specific proposed transformation Y useful?''

\subsection{Concrete Evaluation Workflow}
We now consider the entire workflow involved in choosing concrete
program transformation suggestions to present to a developer. To present
a program transformation tool, researchers would explain decisions made during
the workflow and present evidence about why they made those decisions.

\paragraph{Filtering bad candidates.} Typically, a
program transformation tool generates a large set of candidate
transformations. Depending on the tool, some of these transformations
are inappropriate and need to be filtered out. For instance, a
refactoring tool should only propose semantics-preserving
transformations; on the other hand, we would expect a program repair
tool to change the semantics of the program being repaired.

\paragraph{Selecting good candidates.} In any case, even after ruling out 
inappropriate transformations, the tool would still have a large
number of candidates to consider, and must select some of the
candidates to show to the user. This selection process is a key factor
in the usability of the tool, and is one of the key lessons reported
in the Tricorder
work~\cite{sadowski18:_lesson_build_static_analy_tools_googl}.
Promising tools may fail to be useful in practice because the
selection is not tuned to users' needs.

\paragraph{Evaluating the results.} At this stage, a program
transformation tool would have generated a ranked set of results on a
benchmark.  

A key question is then: which humans should evaluate the
results of the tool?  Possible answers include the researchers
developing the tool; arbitrary developers; or the developers of
particular software projects.

Along with ``which humans'', another key question is how to solicit
information from the humans. Questionnaires and user studies are two
ways to ask solicit information. In both cases, it is difficult to
find an appropriate target audience; ideally, the audience would
provide engaged feedback on the tool, but that is not always obvious.
Questionnaires can be open-ended or closed-ended. Closed-ended
questionnaries can yield easy-to-summarize but coarse information,
while open-ended questionnaires require more effort from participants
to answer and more effort from researchers to analyze. User studies
are more involved than questionnaires but can also provide more information.

At the limit, perhaps the
best way to show that a tool in this space is useful in practice is by
submitting merge requests to maintainers of open-source projects---a 
highly specific and idiosyncratic user study which may be hard to generalize.

\subsection{Examples of evaluations}
We now present some concrete examples of evaluations from the
literature. We start with two language evaluations, and then
discuss examples of questionnaires and other ways of getting feedback. We can observe
here that language evaluations (and in particular the ones we study
here) are not the same as tool evaluations---the language designers
make the design decisions upfront, and there is no analogy to the
filtering-and-selecting steps discussed above.

The $i^*$ modeling framework is a modeling language used in the early
system modeling phase to help understand the problem domain. This
framework has been widely used in research and some industrial
projects. However, Estrada et
al.~\cite{estrada06:_empir_evaluat_framew_model_based} claimed that no
empirical evaluation existed to identify $I^*$'s strengths and
weaknesses. They presented an empirical evaluation of the $i^*$
framework using industrial case studies, conducting their work in
collaboration with an industrial partner. Estrada et al.~report
lessons learned from this experience showing the strengths and
weaknesses of $i^*$, which they hope will play a crucial role in
guiding extensions of the $i^*$ framework.


Another method for getting feedback on the quality of a software tool
and qualitative evaluation is manual self-assessment of results by the
tool developers. This method does not need external help, and the tool
developers can do as detailed an examination of their results as they
desire.  However, it could be vulnerable to potential unconscious bias
and incorrect assumptions by tool developers about what factors would be
important to actual users. Still, it is a necessary first step in tool
evaluation, before presenting the tool to an external audience.

Potential or actual tool users are the gold standard audience for
obtaining valuable feedback. Submitting merge requests to open source
projects is one way to ask for real feedback. Their acceptance (or
not) reveal a ground truth result about whether a project developer
considers the tool useful (though on a case that was hand curated by
the tool developers). Nevertheless, they provide some information
about which factors are the most crucial for the quality of a tool
from the potential customers' point of view. Doing a good job here
does take time.

When getting external feedback about a tool (e.g. questionnaires or
submitting merge requests), tool developers must consider ethics
(including Institutional Review Board approval). Violations of ethical
standards can cause irremediable consequences. For instance, the
University of Minnesota got a university-wide ban for contributing to
the Linux kernel. One of their systems security researchers submitted
merge requests to the Linux kernel for a hidden purpose that they did
not state, which the Linux Foundation deemed
unethical. Linux developers were offended that the university had purposely
wasted the reviewers' time. This resulted in a university-wide ban
following an email from Linux Foundation fellow Greg Kroah-Hartman
which stated: ``I suggest you find a different community to do
experiments on,'' and ``You are not welcome
here.''~\cite{minnesota_banned}


%We will discuss best practices for this type of evaluation.


%% ``An unsound claim can misdirect a field, encouraging the pursuit of unworthy ideas and the abandonment of promising ideas.''. S. M. Blackburn et al.~\cite{truth_Blackburn} believed that having a methodical approach to exploring, exposing, and addressing the root of unsound claims and poor exposition would help to solve this issue. 

% more IR, when there is ground truth
%% Linking user bug reports and code changes for fixing those bugs are missing for several software projects since the bug tracking and version control systems are often maintained independently. There have been some solutions, such as ReLink~\cite{relink}, proposed for this problem. However, Bissyand√© et al.~\cite{ee_buglinking} believed that the presentation of the effectiveness of ReLink is subject to several issues, including a reliability issue with their ground truth datasets in addition to the extent of their measurement. They proposed a benchmark for evaluating this bug-linking solution, and they designed some research questions for quantitative and qualitative assessment of the effectiveness of this tool. Furthermore, they applied their benchmark to ReLink to determine the strengths and limitations of this tool.

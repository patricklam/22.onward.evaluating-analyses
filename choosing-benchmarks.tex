\section{Selecting Benchmarks}
\label{sec:selecting-benchmarks}
What we find depends greatly on where we look---an aphorism that also applies to benchmark selection when evaluating programming languages and software engineering research. 
It might seem that the amount of open-source code available on the Internet today is boundless. However, the second author has observed that their graduate students find it challenging to define suitable benchmark suites to evaluate their research. 
In this section, we discuss tips that we have found to be useful when choosing benchmarks, including filters that are useful to apply to benchmark selection.

\subsection{Existing Benchmark Collections}
Industrial consortia and researchers have created collections of benchmarks for various purposes. SPEC, the Standard Performance Evaluation Corporation, contributed an early Java benchmark collection, SPEC JVM 98~\cite{SPECjvm98} and updated it in 2008~\cite{SPECjvm2008}. SPEC JVM was designed to measure the performance of both Java virtual machine client platforms and hardware systems. Later, researchers introduced the DaCapo benchmark collection~\cite{DaCapo_inproceedings}, consisting of open-source, real-world Java applications. 
They introduced new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. These benchmarks aimed to empirically evaluate performance and memory management efficiency of Java virtual machines.

From a more software engineering point of view, the Qualitas Corpus~\cite{QualitasCorpus} is, per the authors, ``[\ldots] a curated collection of software systems intended for analyzing empirical studies of code artifacts with the primary goal of providing a resource that supports reproducible studies of software.''. However, Qualitas's open-source Java software systems are not necessarily executable. Thus, XCorpus~\cite{XCorpus} is ``a set of 76 executable, real-world Java programs, including a subset of Qualitas Corpus''. To support execution, ``[XCorpus] uses a harness that is a combination of built-in and generated test cases, resulting in a branch coverage that is significantly better than what is available from DaCapo.''.

\subsection{Filters}
Different benchmarks are going to be more or less suitable for exploring different aspects of tools. We acknowledge that being too selective with benchmarks can amount to unfair cherry-picking. However, we claim that applying principled and defensible filters to a large initial benchmark set can be a reasonable way to proceed. Indeed, one might legitimately apply different filters for different intended uses of the benchmarks: if simply collecting quantitative data, one could cast the net broadly and include almost all of the benchmarks. (For instance, when evaluating compiler performance and aiming to produce valid claims, one should usually not cherry-pick benchmarks.) On the other hand, for labour-intensive case studies where one is hoping to submit pull requests, one would want to filter aggressively---it would be desirable to filter as objectively as possible, but being inclusive is not as important. Documenting the filtering criteria would be a best practice, though.

We discuss three filters that are useful to consider when soliciting certain types of feedback. Even if a project fails a filter, it might be good for certain types of evaluation. For instance, benchmark Joda-Time is in maintenance mode, but would still be suitable for developing potential refactorings to be evaluated by us, or by third parties through a questionnaire.

\paragraph{Maintenance Mode}
When proposing code improvements to the developers, one should check that a candidate benchmark is not in maintenance mode. For instance, after submitting the pull request for our Gson benchmark, we received the following feedback: ``This project is in maintenance mode, and we are generally going to be reluctant to accept PRs that are essentially cosmetic, especially if it is not trivially obvious that they do not change anything.''. We also prepared a pull request for Joda-Time, but realized that the project is in maintenance mode before submitting it.

We used the following heuristics to determine whether a project is in maintenance mode.
\begin{enumerate}
  \item Examine the last few commits to the codebase and check whether they are adding new features.
  \item Examine the time gap between the last few commits to see how frequently the codebase is being updated.
  \item Investigate if the developers mention maintenance mode in, for example, recent pull request messages.
\end{enumerate}

For instance, for Joda-Time, the most recent commit was about four months before we looked; the last few commits were all about a release (of a maintenance version); and the pull request message pointed out that they are in maintenance mode.

\paragraph{Estimated Response Time}
Another useful filter is to select projects with recent track records of fast responses to pull requests. For instance, Gson developers provided feedback to our pull request in less than an hour, whereas Bootique developers did not react to our pull request even after a month.

To evaluate the track record, one can look at a project's list of open/closed pull requests. The last few closed pull requests and some open pull requests will provide a reasonable estimate about how long it takes for developers to come up with feedback. Furthermore, the number of contributors can be an indicator too (one might expect a larger contributor pool to be in a position to provide faster feedback).

\paragraph{Familiarity with Benchmark's Domain}
To be able to best evaluate proposed changes to a project, familiarity with the domain of the project can useful, especially when changes are not guaranteed to be semantics-preserving (e.g. when writing synthesis or repair tools). It is unethical to submit known-bad changes; we would argue that researchers should be making a good-faith effort to ensure that their changes are correct and improve the target codebase. Understanding the codebase helps with that.

In our case, we wanted to rate potential refactorings; understanding the context of the potential refactoring was helpful.

%--- edited to here

\subsection{Constraints}

The JTestParametrizer tool forced some conditions on the candidate benchmarks, and I added some more conditions to either simplify the proce
ss of running the benchmarks or increase the potential quality of the feedback. Due to these constraints and because I knew that the feedbac
k I was looking for was mainly on the quality of the refactorings and not quantitative metrics such as performance, I decided to create a ne
w specific set of benchmarks for this work.

Here, I will discuss the main constraints on benchmarks for this part. The JTestParametrizer tool required:
\begin{enumerate}
  \item That the benchmarks be Java projects because the JTestParametrizer tool is designed for the Java programming language.
  \item That the benchmarks run and pass all the test runs successfully on my machines (I used a macOS machine to run the JTestParametrizer 
tool and an Ubuntu machine for running Deckard~\cite{DECKARD}).
  \item That the benchmarks run successfully on Eclipse because the JTestParametrizer is an Eclipse-based tool.
\end{enumerate}

Furthermore, here are three constraints that I added to simplify the process of running the benchmarks or increase the potential value of th
e feedback on the quality of refactorings:
\begin{enumerate}
  \item I added a constraint to use a setup based on the Maven project management tool. Having this constraint enables having the same build
 setup process for all of the benchmarks, which will lead to simplicity and consistency of the process. However, it limits the benchmarks to
 Maven projects.
  \item I added another constraint to restrict the minimum number of stars and forks on GitHub for candidate benchmarks to have more relevan
t and widely used benchmarks. Stars and forks are indicators of how many people decide to use or work on a specific project, and even though
 they are not the perfect metrics, to some degree, they show the reliability and the quality of that project and its developers. Therefore, 
this constraint can enhance the potential quality of feedback from the developers. Nevertheless, it limits the benchmarks to project with a 
certain minimum number of stars and forks.
  \item I added another constraint to discard the benchmarks that took more than one hour to run all the test runs on the macOS machine. Eac
h benchmark will be run repeatedly, and using the projects that take longer to build has no particular advantage to make up for the unnecess
ary time it takes. Therefore, this constraint will help to simplify the process of running benchmarks by saving time. However, it adds a tim
e constraint on the possible candidate benchmarks.
\end{enumerate}

\subsection{Process of Choosing the Benchmarks}

To find candidate benchmarks that satisfy the stated constraints, I started with the benchmarks that Jun Zhao used in his thesis~\cite{ZhaoJ
un2018}. And then, I went through over a hundred open source projects to find the candidates that fit our requirements.


I used the suggested Java projects on GitHub lists by \href{https://medium.com/issuehunt/50-top-java-projects-on-github-adbfe9f67dbc}{IssueH
unt}, \href{https://awesomeopensource.com/projects/maven-plugin}{awesomeopensource}, and \href{https://www.overops.com/blog/the-hitchhikers-
guide-to-github-13-java-projects-you-should-try/}{Henn Idan} to find potential open-source Java project candidates, although I discarded the
 Github projects where their number of stars plus their number of forks on GitHub was less than 800.

I also considered all the open-source Java projects from the \href{https://github.com/google/?q=&type=&language=java&sort=stargazers}{Google
}, \href{https://github.com/spotify/?q=&type=&language=java&sort=stargazers}{Spotify}, \href{https://github.com/apache/?q=&type=&language=java&sort=stargazers}{Apache}, \href{https://github.com/airbnb/?q=&type=&language=java&sort=stargazers}{Airbnb}, and \href{https://github.com/Netflix?q=&type=&language=java&sort=stargazers}{Netflix} companies on GitHub that had the minimum number of stars $+$ forks as potential benchmark candidates.

\subsection{Benchmark Requirements Checklist}

Checklists are a helpful way for quickly evaluating things. They help to be more organized and to not skip any vital step in the process.  I used the following checklist to determine if a candidate benchmark has the minimum requirements needed to be in the benchmark collection for this work.


\begin{enumerate}
  \item Ensure that the candidate is an open-source Java project built by Maven.
  \item Ensure that the candidate contains tests.
  \item Ensure that I can build the candidate on my machines (macOS and Ubuntu machines) and that all the test runs pass successfully; if this is not the case, spend up to one hour fixing the issues. If I cannot fix it, discard this candidate.
  \item Ensure that I can build the candidate on Eclipse; if this is not the case, spend one hour fixing the issues. If I cannot fix it, discard this candidate.
  \item Create the cluster files for that project using Deckard, and then use the cluster files to create the XLS file of the potential nominees for refactoring. Now, discard this candidate if the created XLS file is empty, meaning that there are no nominees for refactoring.
\end{enumerate}

If a project satisfied all these conditions, then I could consider it as a potential benchmark. However, these were the minimum requirements, and some projects had these requirements but still failed to make the benchmark collection due to having a low number of refactoring nominees or not having any refactored cases after running the JTestPrametrizer tool.


\subsection{Deckard and Potential Nominees for Refactoring} \label{section:deckard_explanation}

One of the primary inputs of the JTestParametrizer tool for every benchmark is an XLS file that includes basic information (folder, class, package, method, start line, end line, and clone group size) about the potential nominees for refactoring in the benchmark. I created this XLS file by running a process on the cluster files that I receive from running Deckard's clone detection tool on that benchmark.

To use Deckard's clone detection ability, we need to set up specific parameters such as {\sc Min\_tokens} (``minimum number of tokens required for clones''), {\sc Stride} (``size of the sliding window''), and {\sc Similarity} described in Deckard: scalable and accurate tree-based detection of code clones~\cite{DECKARD}.

Jun Zhao did not document the parameters he used in his research. I deduce that he used 0 for {\sc Stride} (equivalent to no merging of vectors), but I could not deduce what he used for {\sc Min\_tokens} and {\sc Similarity}. Increasing the {\sc Min\_tokens} will decrease the number of potential nominees for refactoring since the nominees have to have at least that many tokens to be eligible. On the other hand, if the {\sc Min\_tokens} is too low, we will have nominees that can be refactored, but the refactoring would not be worth doing since the nominees were very trivial. Furthermore, {\sc Similarity} should be a number between 0.9 and 1. If it is too high, we will miss several nominees (false negatives), and if it is too low, we will have poor nominees (false positives).

After considering eight different sets of values, {\sc Min\_tokens}=50--100, {\sc Stride}=2--0, and {\sc Similarity}=0.95--0.90, for these three parameters for the three benchmarks Gson, Joda-time, and Jfreechart, and evaluating the differences between the final XLS files created by each set of variables, I decided to use 50 for the {\sc Min\_tokens}, 0 for {\sc Stride}, and 0.95 for {\sc Similarity} for every benchmark to keep the results consistent. These chosen parameters produced good results for the three benchmarks that I was using for evaluation.

With 50 for {\sc Min\_tokens}, we will not have many false positives (nominees that can be refactored, but it would be better if not, because they are already trivial), and 50 is not too high to miss good nominees because of the eligibility constraint. Furthermore, Jiang et al.~\cite{DECKARD} state that for clone detection, {\sc Similarity} could be a number between 0.9 and 1, and with 0.95, it is not too high to miss on several nominees, and it is not too low to have poor nominees. Also, for the three benchmarks discussed above, changing the Similarity from 0.9 to 0.95 did not considerably impact the final list of nominees.

\subsection{Our Benchmark Suite}

In this section, I will go over the projects that I used as benchmarks for this work. First, I will go over the numeric facts of each benchmark in table \ref{table:benchmarks}, then I will explain each of the benchmarks briefly.

\paragraph{Numeric Facts}

Table \ref{table:benchmarks} lists the numbers related to the essential aspects of the GitHub repository, including the number of stars, number of forks, number of GitHub issues, number of contributors, number of closed pull requests, and number of open pull requests, on GitHub, for each of the projects used as benchmarks. As for the number of lines of java code, I used SLOCCount\footnote{Source Lines of Code Count, \url{https://dwheeler.com/sloccount/}} to measure it for the latest version of each benchmark.

There are 14 repositories in this table but 18 benchmarks in total. This is because two of the benchmarks (Netty/Codec-http and Netty/Buffer) are subprojects of the same GitHub repository, Netty. Also, I used two different versions of each of the three repositories Gson, Bootique, and Joda-time, as separate benchmarks.


\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l r r r r r r r} 
 Repository & Stars & Forks & Lines of Java & GitHub Issues & Contributors & Closed PRs & Open PRs \\ [0.5ex] 
 \hline\hline
 Gson & 20k & 3.9k & 25.6k & 503 & 114 & 358 & 151 \\ 
 Jimfs & 2k & 245 & 17.4k & 26 & 23 & 96 & 4 \\
 Bootique & 1.3k & 286 & 18.5k & 31 & 17 & 84 & 4 \\
 Joda-time & 4.7k & 888 & 86.5k & 23 & 77 & 160 & 3 \\
 Commons-lang & 2.2k & 1.3k & 85.1k & - & 161 & 682 & 107 \\ 
 Commons-io & 800 & 519 & 41.5k & - & 76 & 232 & 29 \\
 Commons-collections & 506 & 339 & 67.6k & - & 57 & 215 & 28 \\
 Jfreechart & 732 & 296 & 133.5k & 65 & 22 & 75 & 65 \\
 Netty & 27.4k & 13.5k & 312.1k & 451 & 531 & 5,963 & 43 \\ 
 Checkstyle & 6.2k & 8k & 286.5k & 642 & 290 & 6,530 & 41 \\
 Git-commit-id-maven-plugin & 1.3k & 253 & 3.7k & 23 & 73 & 241 & 3 \\
 Docker-maven-plugin & 2.6k & 556 & 2.5k & 10 & 38 & 160 & 11 \\
 Maven & 2.7k & 2k & 91.9k & - & 143 & 428 & 55 \\
 Mybatis-3 & 16.1k & 10.9k & 60.8k & 123 & 182 & 1,156 & 55 \\ [1ex] 
\end{tabular}}
\caption{Numeric Characteristics of Benchmarks}
\label{table:benchmarks}
\end{table}

Table \ref{table:refactoring_nominees} lists the necessary information that we retrieved for each benchmark before running the JTestParametrizer tool on that benchmark. This information includes the version of that benchmark, the number of Java code lines for that specific version using SLOCCount, the number of tests run, failures, errors, and skipped from building that benchmark and running all the tests on my macOS machine, and finally, the number of refactoring nominees that I got from running a process on the cluster files that I got from running the Deckard clone detection tool on that version of the benchmark.

\begin{table}[h!]
\centering
\resizebox{\textwidth}{!}{
\begin{tabular}{l c r r r r r r} 
 Repository & Version & Lines of Java & Tests run & Failures & Errors & Skipped & Nominees \\ [0.5ex] 
 \hline\hline
 Gson & f649e05 & 25193 & 1050 & 0 & 0 & 1 & 39 \\ 
 Gson & f319c1b & 25269 & 1063 & 0 & 0 & 1 & 42 \\ 
 Jimfs & 3c9d8ba & 17472 & 5834 & 0 & 0 & 0 & 45 \\
 Bootique & d0648eb & 18589 & 231 & 0 & 0 & 0 & 22 \\
 Bootique & 9939bc6 & 18591 & 228 & 0 & 0 & 0 & 23 \\
 Joda-time & 0ae5311 & 86138 & 4222 & 0 & 0 & 0 & 261 \\
 Joda-time & 27edfff & 86536 & 4238 & 0 & 0 & 0 & 260 \\
 Commons-lang & 425d808 & 77224 & 4068 & 0 & 0 & 5 & 154 \\ 
 Commons-io & e4ff4a5 & 40336 & 1852 & 0 & 0 & 6 & 32 \\
 Commons-collections & 7d8b979 & 67647 & 16923 & 0 & 0 & 4 & 47 \\
 Jfreechart & d03e68a & 132452 & 2176 & 0 & 0 & 0 & 124 \\
 Netty/Codec-http & e69107c & 41014 & 858 & 0 & 0 & 0 & 47 \\ 
 Netty/Buffer & e69107c & 33564 & 10458 & 0 & 0 & 1198 & 20 \\ 
 Checkstyle & 6cbc1dc & 255741 & 3528 & 0 & 0 & 0 & 130 \\
 Git-commit-id-maven-plugin & 4a1ac8f & 7238 & 214 & 0 & 0 & 1 & 4 \\
 Docker-maven-plugin & 84020ac & 2434 & 59 & 0 & 0 & 0 & 7 \\
 Maven/Maven-core & 3fabb63 & 38653 & 388 & 0 & 0 & 4 & 26 \\
 Mybatis-3 & 1d82865 & 60825 & 1675 & 0 & 0 & 14 & 26 \\ [1ex] 
\end{tabular}}
\caption{Benchmarks' Refactoring Nominees}
\label{table:refactoring_nominees}
\end{table}

\subsection{Benchmarks for Different Types of Feedback}

As we previously discussed in chapter \ref{chap:evaluating}, we were aiming for different types of feedback.

Besides the three necessary constraints and the three unnecessary ones discussed earlier, we learned some factors that led us to create new optional constraints throughout the work. These newly created constraints were not necessarily for all benchmarks and were based on the type of feedback we wanted for that benchmark.


A refactoring case might be correct, meaning that it does not cause any errors or failures, and the behavior of the refactored test case stays the same, but that refactoring case might still be poor due to decreasing the maintainability or readability of the code. In that sense, verifying the correctness of refactorings and validating their quality are two different topics.

I used all 18 benchmarks to get numerical metrics and statistics. Besides, I used Gson, Joda-time, Jfreechart, Commons-lang, Bootique, and Jimfs benchmarks to verify the JTestParametrizer tool's correctness and to find the errors and bugs of the JTestParametrizer tool. If an error occurred when running the JTestParametrizer tool on a benchmark, or if after running the JTestParametrizer on a benchmark that benchmark had some compile errors, or if after running the JTestParametrizer on a benchmark, some of the tests run of that benchmark resulted in failures or errors, then I documented those incidents, and in most cases, tried to find the source of that problem and fix the JTestParametrizer tool.

I used Gson and Joda-time to develop practical examples for the questionnaire to validate the quality of the refactoring instead of the correctness (all the examples were correct), which we ended up not pursuing due to the reasons explained in chapter \ref{chap:qualitative}.

We manually validated refactoring quality for the benchmarks Jimfs, Gson, Joda-time, and Bootique. We also sought developer feedback about test refactorings of these benchmarks using pull requests.

We did not use all the benchmarks the same way. There were some benchmarks such as Checkstyle that we only used for getting numerical metrics and quantitative evaluation. Whereas, there were benchmarks like Gson that we used not only for that purpose but also for coming up with practical examples for the questionnaire, validating the quality of the refactorings by ourselves, and validating the quality of the refactorings by their developers.

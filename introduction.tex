\section{Introduction}
Researchers and practitioners have developed tools to carry out
automated and semi-automated program transformations.  Examples
include tools for bug finding, program repair, and even 
program synthesis (to some extent). These tools work in a large space of possibilities:
for instance, programs invariably have many bugs, but developers only have resources
to address a subset of the bugs.
In more general terms, programming languages research aims to develop
techniques to reason about and manipulate programs. Evaluating
techniques for soundness can be tricky, but is in principle
objective. Soundness, however, is insufficient for a contribution to
be strong. One can additionally evaluate these techniques in terms of
power, or beauty. But often, the (subjective) usefulness of the tool or technique
is a key factor in its value. Techniques may be useful to other researchers, or to
end users. And usefulness can be supported by rhetoric or by
experiment. 

Evaluating tools in this space is often subjective---there is no
single right answer. Some tools, such as program refactoring tools,
are designed to only affect nonfunctional properties of the
software---particularly maintainability---but different users have
different standards for maintainability. Program synthesis is also an
underconstrained problem and humans must decide which one of many
potential solutions is most appropriate. And, program repair
tools attempt to make the software (closer to) correct. Even in 
the bug detection space: although, per 
\citeN{hovemeyer04:_findin_bugs_easy}, ``finding bugs is
easy'', deciding which bugs are
important to fix (and don't cause undesired regressions) is hard and,
in the end, requires a judgment call by a human.

The goal of this work is to discuss best practices for evaluating
program transformation tools. We use best practices from the literature
to inform our arguments, as well as our recent experience developing
a specific tool, JTestParametrizer. That tool proposes
refactorings based on static analysis results, and we describe some of the
steps that we have taken to evaluate the usefulness of our tool.

We point out that our focus here is on evaluating program transformation \emph{tools}.
We claim that the main question, which we will elaborate on in Section~\ref{sec:how-to-evaluate}, is:

\begin{center}
\fbox{
\begin{minipage}{.6\columnwidth}
\emph{How} is the tool being evaluated, \\
on \emph{which subjects},\\
and by~\emph{who}?
\end{minipage}
}
\end{center}

There is a small body of work on evaluating languages rather than tools.
\citeN{coblenz18:_inter_progr_languag_desig} 
discussed big-picture issues with respect to how to design and evaluate programming languages, and followed up with a specific case study focussing on their Obsidian
language~\cite{coblenz20:_can_advan_type_system_be_usabl}.
Earlier, \citeN{markstrum10:_stakin_claim} noted that programming 
language design claims were typically under-supported, and \citeN{halverson09:_climb_plateau} explored how to evaluate programmer productivity in a
proposed language. 

While both programming languages and program transformation tools exist
in vast open-ended design spaces, we claim that the design of program transformation
tools fundamentally aim to serve already-existing codebases, while programming 
language design aims to serve codebases to be written in the future.
This difference implies that appropriate evaluation for language is different 
from appropriate evaluation for transformation tools.
(Migration tools bridge the gap between new programming language designs
and existing codebases).


\section{Introduction}
Researchers and practitioners have developed tools to carry out
automated and semi-automated program transformations.  Examples
include tools for bug finding, program repair, and even 
program synthesis (to some extent). These tools work in a vast space of possibilities:
for instance, programs invariably have many bugs, but developers only have resources
to address a subset of the bugs.
In more general terms, programming languages research aims to develop
techniques to reason about and manipulate programs. Evaluating
techniques for soundness can be tricky, but is in principle
objective. Soundness, however, is insufficient for a contribution to
be strong. One can additionally evaluate these techniques in terms of
expressive power, or beauty. But often, the (subjective) usefulness of the tool or technique
is a key factor in its value. Techniques may be useful to other researchers, or to
end users. And usefulness can be supported by rhetoric or by
experiment. 

Evaluating tools in this space is often subjective---there is no
single right answer. Some tools, such as program refactoring tools~\cite{ivers22:_indus_cry_tools_suppor_large_scale_refac},
are designed to only affect nonfunctional properties of the
software, e.g. maintainability, but different users have
different standards for maintainability. Program synthesis~\cite{solar-lezama08:_progr_synth_sketc} is also an
underconstrained problem and humans must set policies that decide which solution, of many
potential solutions, is most appropriate. And, program repair
tools~\cite{griesmayer06:_repair_boolean_progr_applic_c} 
attempt to make the software (closer to) correct. Even in 
the bug detection space: although, per 
\citeN{hovemeyer04:_findin_bugs_easy}, ``finding bugs is
easy'', deciding which bugs are
important to fix (and don't cause undesired regressions) is hard and,
in the end, requires a judgment call by an experienced human.

Our aim in writing this work is twofold. We seek to:
\begin{itemize}
\item spark a discussion of best practices for evaluating
source-to-source program transformation tools;
\item as a case study, present our experience evaluating a particular 
tool---our JTestParametrizer test refactoring tool.
\end{itemize}

Typically, a program
transformation tool~\cite{visser04:_progr_trans} accepts a source program (written in a programming
language that the tool recognizes) and proposes modifications to the source code,
purportedly improving it in some way. Stratego/XT~\cite{bravenboer08:_strat_xt},
which is now part of the Spoofax language workbench, is an influential language
and toolset for developing program transformation tools.
We use best practices from the literature
to inform our proposed best practices, as well as our recent experience developing
a specific tool, JTestParametrizer. That tool proposes
refactorings based on static analysis results, and we describe some of the
steps that we have taken to evaluate the usefulness of our tool.

While both programming languages and program transformation tools exist
in vast open-ended design spaces, we claim that the design of program transformation
tools fundamentally aim to serve already-existing codebases, while programming 
language design aims to serve codebases to be written in the future.
This difference implies that appropriate evaluation for language is different 
from appropriate evaluation for transformation tools.
(Migration tools like Gofix~\cite{cox11:_introd_gofix} bridge the gap between new programming language designs
and existing codebases).

Our focus here is on evaluating program transformation \emph{tools}.
We claim that the main evaluation question, which we will elaborate on in Section~\ref{sec:how-to-evaluate}, is:

\begin{center}
\fbox{
\begin{minipage}{.6\columnwidth}
\emph{How} is the tool being evaluated, \\
on \emph{which subjects},\\
and by~\emph{who}?
\end{minipage}
}
\end{center}

We next discuss evaluation in related domains in Section~\ref{sec:related}.
Reproducibility strengthens claims about systems, and Section~\ref{sec:reproducibility}
surveys reproducibility. Moving on to tool evaluation, Section~\ref{sec:how-to-evaluate} 
expands on how tool developers can support claims about their tools, by enumerating
types of evaluations and listing classes of participants that can be recruited to
provide opinions. Section~\ref{sec:selecting-benchmarks} completes the evaluation picture
by surveying sources of systems that can serve as benchmarks and listing filters
that may be helpful in reducing the number of systems to consider. Finally, 
Section~\ref{sec:our-evaluation-process} presents both how we performed an evaluation of 
JTestParametrizer and also summarizes examples of other evaluations from the literature.


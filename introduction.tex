\section{Introduction}
Researchers and practitioners have developed tools to carry out
automated and semi-automated program transformations.  Examples
include tools for bug finding, program repair, and even 
program synthesis (to some extent). These tools work in a large space of possibilities:
for instance, programs have many potential bugs, only some of which
matter.
% XXX citations
In more general terms, programming languages research aims to develop
techniques to reason about and manipulate programs. Evaluating
techniques for soundness can be tricky, but is in principle
objective. Soundness, however, is insufficient for a contribution to
be strong. One can additionally evaluate these techniques in terms of
power, or beauty. But often, the usefulness of the tool or technique
is a key factor in its value, which can be shown
experimentally. Techniques may be useful to other researchers, or to
end users.

The goal of this paper is to discuss best practices for evaluating
program transformation tools. When appropriate, we use our
JTestParametrizer tool as a case study. That tool proposes
refactorings based on static analysis results; we describe some of the
steps that we have taken to evaluate the usefulness of our tool.

Evaluating tools in this space is often subjective---there is no
single right answer. Some tools, such as program refactoring tools,
are designed to only affect nonfunctional properties of the
software---particularly maintainability---yet different users have
different standards for maintainability. Program synthesis is also an
underconstrained problem and humans must decide which one of many
potential solutions is most appropriate. And, program repair
tools attempt to make the software (closer to) correct. Even in that
space: although, per \citeN{hovemeyer04:_findin_bugs_easy}, ``finding bugs is
easy'', deciding which bugs are
important to fix (and don't cause undesired regressions) is hard and
requires a judgment call. 

We point out that our focus here is on evaluating program transformation \emph{tools}.
There is a small body of work on evaluating languages rather than tools.
\citeN{coblenz18:_inter_progr_languag_desig} 
discussed big-picture issues with respect to how to design and evaluate programming languages, and followed up with a specific case study focussing on their Obsidian
language~\cite{coblenz20:_can_advan_type_system_be_usabl}.
Earlier, \citeN{markstrum10:_stakin_claim} noted that programming 
language design claims were typically under-supported, and \citeN{halverson09:_climb_plateau} explored how to evaluate programmer productivity in a
proposed language. 

While both programming languages and program transformation tools exist
in vast open-ended design spaces, we argue that the design of program transformation
tools fundamentally aim to serve already-existing codebases; programming 
language design aims to serve codebases to be written in the future.
(Migration tools bridge the gap between new programming language designs
and existing codebases).


\section{Related Work}
\label{sec:related}
%% We continue our discussion of evaluations from the Introduction, where we 
%% discussed programming language designs. We now
%% discuss evaluations of language implementations and of visualization
%% tools.
There is a small body of work on evaluating languages rather than tools.
But first, we point out~\citeN{hoare74:_hints}'s work on programming language
design. In that work, Hoare uses rhetoric and experience, but not experiments,
to support claims about programming language design. More recently,
there was a Dagstuhl workshop ``Evidence About Programmers for Programming
Language Design''~\cite{stefik18:_eviden_about_progr_progr_languag_desig} which discussed, in part, the role of evaluations in programming language design.


\citeN{coblenz18:_inter_progr_languag_desig} 
discussed big-picture issues with respect to how to design and evaluate programming languages, and followed up with a specific case study focussing on their Obsidian
language~\cite{coblenz20:_can_advan_type_system_be_usabl}.
Earlier, \citeN{markstrum10:_stakin_claim} noted that programming 
language design claims were typically under-supported, and \citeN{halverson09:_climb_plateau} explored how to evaluate programmer productivity in a
proposed language. 

To our knowledge, the first empirical evaluations in the programming
languages fields were of programming language implementations
(e.g. compilers, and, later, Just-in-Time engines); 
SPEC was charged with curating impartial benchmark suites,
which would evaluate the performance of
computer systems and programming language implementations. 
The influence of this early focus continues
today, with Blackburn and 17 other prominent programming languages
researchers
authoring~\cite{blackburn16:_truth_whole_truth_nothin_but_truth}. That
work describes errors (``sins'') in empirical evaluation of programming
language \emph{implementations and systems}.  Specifically, it proposed a
framework that identifies two categories of sins: sins of reasoning,
leading to unsound claims; and sins of exposition, leading to poorly
specified claims and evaluations. Their framework (and 
checklist~\cite{berger19:_check_manif_empir_evaluat}) provides
practitioners with methodological techniques for evaluating the
integrity of their work or the work of others.  However, Blackburn et al's
recommendations are most appropriate for
performance-related evaluations of implementations. 
The present work focusses more on holistic
evaluations of designs of program transformation tools.

We are not aware of any other work surveying evaluations in the program
transformation tools space, though~\citeN{visser04:_progr_trans}
provided a taxonomy of such tools at \url{https://www.program-transformation.org/}.
\citeN{kahani19:_survey_class_model_trans_tools} classify the
\emph{declared} capabilities of 60 model transformation tools. Unlike
the present work, they do not discuss the quality of the evidence
behind each of the model transformation tools; our aim here is instead
to discuss best practices for showing usefulness of program
transformation tools.

Program transformation tools can be viewed as software engineering
tools, and software engineering research often involves sampling. 
\citeN{baltesar:_sampl_softw_engin_resear} survey recent
published works in software engineering and investigate how these works sample code
artifacts, people, and non-code artifacts.  They recommend best
practices for sampling to ensure generalizability of research results.

Moving farther afield from the field of programming languages, but
much closer to our present goals,
\citeN{merino18:_system_liter_review_softw_visual_evaluat} survey
published work about software visualization tools. As with
program transformation tools, software visualization tools operate in
an open-ended space where it is almost impossible to make clear
quantitative judgments of work. They investigate evaluation strategies
for visualization tools, including survey-based, anecdotal evidence,
usage scenarios/case studies, experiments, and examples; and data
collection methods including questionnaires, think-aloud, interview,
video recording, sketch drawing, and others. They propose guidelines
for future work in that space, recommending the use of case studies
and experiments that can show effectiveness of novel tools.

User experience (UX) also needs to be evaluated by people, although
criteria there often differ from our primary criterion of
usefulness. \citeN{vermeeren10:_user_exper_evaluat_method} collect 96
methods for evaluating UX and how humans feel while using products.
UX also aims to go beyond usability measures and investigate
how a product affects a human before, during, and after the interaction.

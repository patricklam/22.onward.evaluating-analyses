\section{Reproducibility}
\label{sec:reproducibility}

A discussion of evaluation presupposes reproducible runs of the
underlying artifact. Science can only work if results are
reproducible.  The challenges in performing reproducible scientific
research range from technical issues concerning input data,
under-specification in methodology or metrics, obfuscated or
unavailable codebases, and selective or exaggerated
reporting~\cite{allison2018reproducibility}. As a field, computer
science has historically not valued reproducibility; \citeN{bajpai2017challenges}
discuss some reasons why, and \citeN{collberg16:_repeat_comput_system_resear}
attempted to empirically measure repeatability.

\citeN{krishnamurthi:_about_artif_evaluat} advocated for artifact evaluation in software research: ``If a paper makes, or implies, claims that require software, those claims must be backed up.''. Programming languages conferences now have artifact evaluation processes and are continuing to refine their processes. Per Krishnamurthi, ``Artifact evaluation encourages authors to produce reasonable artifacts, which are the cornerstone of future research.''.

We have learned that being able to re-run even one's own artifacts years after 
initial development is quite valuable; in a typical research situation,
students come and go, and the institutional knowledge about how to run a
tool may no longer be present, making it hard to reproduce and in particular to build
on one's past results. In this section, we briefly discuss challenges
to reproducibility and some of the ways we have overcome them.

Artifacts run in environments, which include hardware, OS kernels,
distributions, (versioned) packages, configurations, and file system
contents. Differences in the environment hinder redeployment of artifacts,
despite the age-old ``Write Once, Run Anywhere'' dream that was encapsulated in
Sun Microsystems's 1995 slogan.

An early pragmatic attempt\footnote{That is, by accepting the world as-it-is rather than by creating a self-contained insular environment.} to address this problem was \citeN{guo2011cde}'s CDE 
packaging framework, which recreates the file system tree of the source environment and ships the code along with its input data to an arbitrary environment by creating a fake root containing the mentioned artifacts in the destination machine. While Guo's idea of shipping the entire environment as an apparatus for reproducible software runs is interesting, that work only encapsulated the file system. rr\footnote{\url{https://rr-project.org/}} goes further in a particular direction and provides the ability to rerun a specific execution of a binary; however, it does not provide insight into the system as a whole, nor does it facilitate experimentation with related executions of the binary. To be fair, rr's anticipated use case was not research reproducibility.

CDE's packaging strategy has evolved into modern containerization technologies such as Docker and LXC, which use Linux kernel features to replicate an environment as accurately as possible, in isolation from other containers and the host operating system. Thus, containers provide a comprehensive framework for creating the desired environment in machines with different OS distributions and installed packages. 

This has led many researchers to advocate for containers as a solution for reproducible research. For instance, \citeN{boettiger2015introduction} proposed Docker for reproducible research, defining best practices in doing so, e.g., using the containers during development, test cases, and checks, and using Dockerfiles instead of manually typing in commands. Virtualization technologies, in general, can play an essential role in future research by providing a snapshot of the testing environment and the evaluation results. They are no panacea, though, and performance results are particularly ill-suited to reproduction under containers.

Without such arrangements, reproducing studies on programming languages and software evaluations can often be challenging. For instance, \citeN{Berger_Reproducing} follow the footsteps of \citeN{ray2014large} along with their GitHub repository, documenting the results and cross-checking the claims. In this effort, reproducing some of the case studies has proven complex and not been completely successful.

\paragraph{JParametrizer reproducibility case study.}
We make our discussion concrete by discussing our experience with JTestParametrizer. The context was that a previous student had graduated, leaving us the artifact itself, along with a master's thesis documenting the work. 

Fortunately, we were able to obtain the exact versions of the earlier 5 benchmarks, as the evaluated git commit hash of each of the benchmarks was included in the artifact. We also had access to the artifact itself. However, we found that we were missing an intermediate Excel file, which contained a set of potential refactoring nominees. This file was generated by the Deckard tool; unfortunately, the previous student did not document the 3 Deckard parameters used to generate the Excel file. Rerunning JTestParametrizer with a newly-generated Excel file seemed to produce slightly different results from those documented, as the new run produced some compilation errors, while the thesis reported no compilation errors.

In addition to the Deckard dependency, our tool also had a dependency on the Eclipse IDE. So, we relied on the version being available to us now having the same behaviour as an earlier version, and we needed the run configurations to be comparable.

To aid future reproducibility of our work, we created a Figshare article documenting versions and configurations of our tool's dependencies. However, this is still a manual process and subject to human error; in the best case, a second person would attempt to follow the directions, re-run the tool, and compare the results.

In the remainder of this work, we set aside the important question of reproducibility, and focus on how to evaluate the designs and artifacts in question.

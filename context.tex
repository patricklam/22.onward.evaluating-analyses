\section{Setting}
In this work, we focus on tools which carry out program transformations. 
In more general terms, programming languages research aims to develop techniques to reason about and manipulate programs. Evaluating techniques for soundness can be tricky, but is in principle objective. Soundness, however, is insufficient for a contribution to be strong. One can additionally evaluate these techniques in terms of power, or beauty. But often, the usefulness of the tool or technique is a key factor in its value, which can be shown experimentally. Techniques may be useful to other researchers, or to end users.

Our case study is a tool that proposes refactorings based on static analysis results; we aim to evaluate the usefulness of our tool. Other tools that also need to be evaluated for usefulness include tools for bug finding, program repair, and even to some extent program synthesis. These tools work in a large space of possibilities: for instance, programs have many potential bugs, only some of which matter.

One way to show that a tool is useful in practice is by submitting merge requests to maintainers of open-source projects. We will discuss best practices for this sort of evaluation.


\section{Concrete Examples of Evaluations in PL Research}

Linking user bug reports and code changes for fixing those bugs are missing for several software projects since the bug tracking and version control systems are often maintained independently. There have been some solutions, such as ReLink~\cite{relink}, proposed for this problem. However, Bissyand√© et al.~\cite{ee_buglinking} believed that the presentation of the effectiveness of ReLink is subject to several issues, including a reliability issue with their ground truth datasets in addition to the extent of their measurement. They proposed a benchmark for evaluating this bug-linking solution, and they designed some research questions for quantitative and qualitative assessment of the effectiveness of this tool. Furthermore, they applied their benchmark to ReLink to determine the strengths and limitations of this tool.

The $i^*$ modeling framework is a modeling language used in the early system modeling phase to help understand the problem domain. This framework has been widely used in research and some industrial projects. However, Estrada et al.~\cite{ee_framework} concluded that no empirical evaluation existed to identify the strengths and weaknesses of this framework. They presented an empirical evaluation of the $i^*$ framework using industrial case studies. They conducted their work in collaboration with an industrial partner who was using object-oriented and model-driven approaches for their software development. Estrada et al.\ report lessons learned from this experience showing the strengths and weaknesses of this framework and, they believe that this evaluation could play a crucial role in guiding extensions of the $i^*$ framework.

% you should briefly define construct validity in a short phrase
Questionnaires are one method for soliciting feedback. However, a questionnaire does not guarantee quality results because it is difficult to find the right engaged target audience for a technical software tool questionnaire. Furthermore, feedback from the questionnaires tends to have a lower level of detail.
Laugwitz et al.~\cite{laugwitz2008construction} designed a user experience questionnaire to get feedback on six factors: Attractiveness, Perspicuity, Efficiency, Dependability, Stimulation, and Novelty. Their results indicated a satisfying level of reliability and construct validity.

Another method for getting feedback on the quality of a software tool and qualitative evaluation is manual self-assessment. The feedback received from this assessment can be as detailed as required. This method does not need external help, but it could be vulnerable to potential unconscious bias and wrong assumptions about what factors would be the most crucial for the overall quality of the tool.

One of the best techniques to obtain valuable feedback for a specific software tool would be to contact potential customers directly about the quality of the changes that our tool has made to their codebase. Submitting pull requests is a method that makes this technique possible. This allows us to determine which factors are the most crucial for the quality of our tool from the potential customers' point of view. However, using this method to get feedback requires spending more time.

When using the methods that require external help for getting feedback on the overall quality of changes (such as using questionnaires or submitting pull requests), an essential consideration is that ethical standards should be held whenever a method seeks external help. Violations of those ethical standards can cause irremediable consequences.

An example of these consequences occurred when the University of Minnesota got a university-wide ban by the Linux kernel. One of their systems-security researchers submitted pull requests to the Linux kernel for a hidden purpose that they did not state, which the Linux Foundation deemed very unethical. Developers were offended that the university had purposely wasted the reviewers' time. This resulted in a university-wide ban following an email from Linux Foundation fellow Greg Kroah-Hartman which stated: ``I suggest you find a different community to do experiments on,'' and ``You are not welcome here.''~\cite{minnesota_banned}

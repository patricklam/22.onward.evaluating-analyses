\section{Selecting Benchmarks}
\label{sec:selecting-benchmarks}
What we find depends greatly on where we look---an aphorism that also applies to benchmark selection when evaluating programming languages and software engineering research. 
It might seem that the amount of open-source code available on the Internet today is boundless. However, the second author has observed that their graduate students find it challenging to define suitable benchmark suites to evaluate their research. 
In this section, we discuss tips that we have found to be useful when choosing benchmarks, including filters that are useful to apply to benchmark selection.

\subsection{Existing Benchmark Collections}
Industrial consortia and researchers have created collections of benchmarks for various purposes. SPEC, the Standard Performance Evaluation Corporation, contributed an early Java benchmark collection, SPEC JVM 98~\cite{SPECjvm98} and updated it in 2008~\cite{SPECjvm2008}. SPEC JVM was designed to measure the performance of both Java virtual machine client platforms and hardware systems. Later, researchers introduced the DaCapo benchmark collection~\cite{DaCapo_inproceedings}, consisting of open-source, real-world Java applications. 
They introduced new value, time-series, and statistical metrics for static and dynamic properties such as code complexity, code size, heap composition, and pointer mutations. These benchmarks aimed to empirically evaluate performance and memory management efficiency of Java virtual machines.

From a more software engineering point of view, the Qualitas Corpus~\cite{QualitasCorpus} is, per the authors, ``[\ldots] a curated collection of software systems intended for analyzing empirical studies of code artifacts with the primary goal of providing a resource that supports reproducible studies of software.''. However, Qualitas's open-source Java software systems are not necessarily executable. Thus, XCorpus~\cite{XCorpus} is ``a set of 76 executable, real-world Java programs, including a subset of the Qualitas Corpus''. To support execution, ``[XCorpus] uses a harness that is a combination of built-in and generated test cases, resulting in a branch coverage that is significantly better than what is available from DaCapo.'' In other research, we have used the DUETS benchmarks~\cite{durieux21} as a curated collection of pairs of libraries and clients.

\subsection{Filters (generic)}
\label{sec:filters-generic}
Different benchmarks are going to be more or less suitable for exploring different aspects of tools. We acknowledge that being too selective with benchmarks can amount to unfair cherry-picking. However, we claim that applying principled and defensible filters to a large initial benchmark set can be a reasonable way to proceed. Indeed, one might legitimately apply different filters for different intended uses of the benchmarks: if simply collecting quantitative data, one could cast the net broadly and include almost all of the benchmarks. (For instance, when evaluating compiler performance and aiming to produce valid claims, one should usually not cherry-pick benchmarks.) On the other hand, for labour-intensive case studies where one is hoping to submit pull requests, one would want to filter aggressively---it would be desirable to filter as objectively as possible, but being inclusive is not as important. Documenting the filtering criteria would be a best practice, though.

We discuss filters that are useful to consider when soliciting certain types of feedback. Even if a project fails a filter, it might be good for certain types of evaluation. For instance, the Joda-Time library is in maintenance mode, but would still be suitable for developing potential refactorings to be evaluated by us, or by third parties through a questionnaire.

\paragraph{Popularity}
Given a large potential benchmark set, it can be useful to choose projects with many GitHub stars and forks; such projects may be more relevant and widely used, or at least more well-known and potentially more active. As proxies for popularity and activity, choosing highly-starred and -forked projects may result in getting more feedback (from a larger developer pool).

\paragraph{Maintenance Mode}
When proposing code improvements to the developers, one should check that a candidate benchmark is not in maintenance mode. For instance, after submitting the pull request for our Gson benchmark, we received the following feedback: ``This project is in maintenance mode, and we are generally going to be reluctant to accept PRs that are essentially cosmetic, especially if it is not trivially obvious that they do not change anything.''. We also prepared a pull request for Joda-Time, but realized that the project is in maintenance mode before submitting it.

We used the following heuristics to determine whether a project is in maintenance mode.
\begin{enumerate}
  \item Examine the last few commits to the codebase and check whether they are adding new features.
  \item Examine the time gap between the last few commits to see how frequently the codebase is being updated.
  \item Investigate if the developers mention maintenance mode in, for example, recent pull request messages.
\end{enumerate}

For instance, for Joda-Time, the most recent commit was about four months before we looked; the last few commits were all about a release (of a maintenance version); and the pull request message pointed out that they are in maintenance mode.

\paragraph{Estimated Response Time}
Another useful filter is to select projects with recent track records of fast responses to pull requests. \citeN{zhang22:_pull} identify factors explaining pull request latency, but that general type of information is not useful to us here; we want a fine-grained look at the specific project that we target, and its specific recent track record. For instance, Gson developers provided feedback to our pull request in less than an hour, whereas Bootique developers did not react to our pull request even after a month.

To evaluate the track record, one can look at a project's list of open/closed pull requests. The last few closed pull requests and some open pull requests will provide a reasonable estimate about how long it takes for developers to come up with feedback. Furthermore, the number of contributors can be an indicator too (one might expect a larger contributor pool to be in a position to provide faster feedback).

\paragraph{Familiarity with Benchmark's Domain}
To be able to best evaluate proposed changes to a project, familiarity with the domain of the project can useful, especially when changes are not guaranteed to be semantics-preserving (e.g. when writing synthesis or repair tools). It is unethical to submit known-bad changes; we would argue that researchers should be making a good-faith effort to ensure that their changes are correct and improve the target codebase. Understanding the codebase helps with that.

%In our case, we wanted to rate potential refactorings; understanding the context of the potential refactoring was helpful.



